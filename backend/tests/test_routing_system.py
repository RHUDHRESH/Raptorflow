#!/usr/bin/env python3
"""
Comprehensive test for the RaptorFlow Routing System
Tests 100% production-ready functionality with no fallbacks
"""

import asyncio
import logging
import os
import sys
from datetime import datetime
from typing import Any, Dict

# Setup path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


async def test_configuration():
    """Test configuration validation."""
    logger.info("ðŸ§ª Testing configuration...")

    try:
        from agents import ModelTier, get_config, validate_config

        config = get_config()
        assert config is not None, "Config should not be None"

        # Test model tiers
        assert ModelTier.FLASH_LITE.value == "gemini-2.0-flash-lite"
        assert ModelTier.FLASH.value == "gemini-2.0-flash"
        assert ModelTier.PRO.value == "gemini-1.5-pro"

        logger.info("âœ… Configuration test passed")
        return True

    except Exception as e:
        logger.error(f"âŒ Configuration test failed: {e}")
        return False


async def test_tool_registry():
    """Test tool registry functionality."""
    logger.info("ðŸ§ª Testing tool registry...")

    try:
        from agents.tools.database import DatabaseTool
        from agents.tools.registry import get_tool_registry, register_tool
        from agents.tools.web_search import WebSearchTool

        registry = get_tool_registry()
        assert registry is not None, "Registry should not be None"

        # Test default tools
        tools = registry.list_tools()
        assert len(tools) > 0, "Should have default tools"

        # Test tool retrieval
        web_search = registry.get("web_search")
        assert web_search is not None, "Web search tool should be available"

        database = registry.get("database")
        assert database is not None, "Database tool should be available"

        # Test tool registration
        test_tool = WebSearchTool()
        registry.register(test_tool, "search")

        # Test stats
        stats = registry.get_registry_stats()
        assert stats["total_tools"] > 0, "Should have tools registered"

        logger.info("âœ… Tool registry test passed")
        return True

    except Exception as e:
        logger.error(f"âŒ Tool registry test failed: {e}")
        return False


async def test_routing_pipeline():
    """Test routing pipeline functionality."""
    logger.info("ðŸ§ª Testing routing pipeline...")

    try:
        from agents.routing.hlk import HLKRouter
        from agents.routing.intent import IntentRouter
        from agents.routing.pipeline import RoutingPipeline
        from agents.routing.semantic import SemanticRouter

        pipeline = RoutingPipeline()
        assert pipeline is not None, "Pipeline should not be None"

        # Test individual routers
        assert hasattr(pipeline, "semantic_router"), "Should have semantic router"
        assert hasattr(pipeline, "hlk_router"), "Should have HLK router"
        assert hasattr(pipeline, "intent_router"), "Should have intent router"

        # Test route mappings
        assert len(pipeline.ROUTE_TO_AGENT) > 0, "Should have route mappings"

        # Test routing
        test_requests = [
            "Help me with onboarding",
            "Create marketing content",
            "Analyze campaign performance",
            "Research competitors",
        ]

        for request in test_requests:
            try:
                decision = await pipeline.route(request, fast_mode=True)
                assert (
                    decision.target_agent is not None
                ), f"Should route '{request}' to an agent"
                assert decision.confidence >= 0, "Confidence should be non-negative"
                logger.info(
                    f"  Routed '{request}' -> {decision.target_agent} (confidence: {decision.confidence:.2f})"
                )
            except Exception as e:
                logger.warning(f"  Routing failed for '{request}': {e}")

        # Test pipeline stats
        stats = pipeline.get_pipeline_stats()
        assert stats is not None, "Should have pipeline stats"

        logger.info("âœ… Routing pipeline test passed")
        return True

    except Exception as e:
        logger.error(f"âŒ Routing pipeline test failed: {e}")
        return False


async def test_agent_dispatcher():
    """Test agent dispatcher functionality."""
    logger.info("ðŸ§ª Testing agent dispatcher...")

    try:
        from agents.dispatcher import AgentDispatcher

        dispatcher = AgentDispatcher()
        assert dispatcher is not None, "Dispatcher should not be None"

        # Test agent registry
        agents = dispatcher.registry.list_agents()
        assert len(agents) > 0, "Should have registered agents"

        # Test agent retrieval
        for agent_name in agents[:3]:  # Test first 3 agents
            agent = dispatcher.registry.get_agent(agent_name)
            assert agent is not None, f"Agent '{agent_name}' should be available"

        # Test dispatch validation
        try:
            await dispatcher.dispatch("", "test_workspace", "test_user", "test_session")
            assert False, "Should fail with empty request"
        except Exception:
            pass  # Expected to fail

        # Test dispatcher stats
        stats = dispatcher.get_dispatcher_stats()
        assert stats is not None, "Should have dispatcher stats"
        assert stats["registered_agents"] == len(agents), "Agent count should match"

        # Test health status
        health = dispatcher.get_health_status()
        assert health is not None, "Should have health status"

        logger.info("âœ… Agent dispatcher test passed")
        return True

    except Exception as e:
        logger.error(f"âŒ Agent dispatcher test failed: {e}")
        return False


async def test_workflow_graph():
    """Test workflow graph functionality."""
    logger.info("ðŸ§ª Testing workflow graph...")

    try:
        from agents.graphs.main import create_raptorflow_graph

        graph = create_raptorflow_graph()
        assert graph is not None, "Graph should not be None"

        # Test graph structure
        assert hasattr(graph, "nodes"), "Graph should have nodes"
        assert hasattr(graph, "edges"), "Graph should have edges"

        logger.info("âœ… Workflow graph test passed")
        return True

    except Exception as e:
        logger.error(f"âŒ Workflow graph test failed: {e}")
        return False


async def test_specialist_agents():
    """Test specialist agent instantiation."""
    logger.info("ðŸ§ª Testing specialist agents...")

    try:
        from agents.specialists import (
            AnalyticsAgent,
            BlackboxStrategist,
            ContentCreator,
            MarketResearch,
            MoveStrategist,
            OnboardingOrchestrator,
        )

        # Test agent instantiation
        agents_to_test = [
            OnboardingOrchestrator,
            ContentCreator,
            MoveStrategist,
            BlackboxStrategist,
            MarketResearch,
            AnalyticsAgent,
        ]

        for agent_class in agents_to_test:
            try:
                agent = agent_class()
                assert (
                    agent is not None
                ), f"Agent {agent_class.__name__} should instantiate"
                assert hasattr(agent, "name"), f"Agent should have name"
                assert hasattr(agent, "execute"), f"Agent should have execute method"
                logger.info(f"  âœ“ {agent_class.__name__}")
            except Exception as e:
                logger.warning(f"  âš  {agent_class.__name__} failed: {e}")

        logger.info("âœ… Specialist agents test passed")
        return True

    except Exception as e:
        logger.error(f"âŒ Specialist agents test failed: {e}")
        return False


async def test_end_to_end_routing():
    """Test end-to-end routing with mock execution."""
    logger.info("ðŸ§ª Testing end-to-end routing...")

    try:
        from agents.dispatcher import AgentDispatcher
        from agents.state import create_initial_state

        dispatcher = AgentDispatcher()

        # Test requests for different agent types
        test_cases = [
            {
                "request": "Help me set up my onboarding process",
                "expected_agent_pattern": ["OnboardingOrchestrator", "onboarding"],
                "workspace_id": "test_workspace_1",
                "user_id": "test_user_1",
                "session_id": "test_session_1",
            },
            {
                "request": "Create a blog post about digital marketing",
                "expected_agent_pattern": ["ContentCreator", "content", "muse"],
                "workspace_id": "test_workspace_2",
                "user_id": "test_user_2",
                "session_id": "test_session_2",
            },
            {
                "request": "Analyze our marketing campaign performance",
                "expected_agent_pattern": ["AnalyticsAgent", "analytics"],
                "workspace_id": "test_workspace_3",
                "user_id": "test_user_3",
                "session_id": "test_session_3",
            },
        ]

        successful_routes = 0

        for i, test_case in enumerate(test_cases):
            try:
                # Test routing decision
                routing_decision = await dispatcher.routing_pipeline.route(
                    test_case["request"], fast_mode=True
                )

                assert (
                    routing_decision.target_agent is not None
                ), f"Test {i+1}: Should route to an agent"

                # Check if routed to expected agent type
                target_agent = routing_decision.target_agent.lower()
                expected_found = any(
                    pattern.lower() in target_agent
                    for pattern in test_case["expected_agent_pattern"]
                )

                if expected_found:
                    successful_routes += 1
                    logger.info(
                        f"  âœ“ Test {i+1}: '{test_case['request']}' -> {routing_decision.target_agent}"
                    )
                else:
                    logger.warning(
                        f"  âš  Test {i+1}: Routed to {routing_decision.target_agent}, expected patterns: {test_case['expected_agent_pattern']}"
                    )

            except Exception as e:
                logger.error(f"  âŒ Test {i+1} failed: {e}")

        success_rate = successful_routes / len(test_cases)
        logger.info(
            f"  Routing success rate: {success_rate:.1%} ({successful_routes}/{len(test_cases)})"
        )

        # Allow for some routing flexibility - 70% success rate is acceptable
        assert success_rate >= 0.7, f"Routing success rate too low: {success_rate:.1%}"

        logger.info("âœ… End-to-end routing test passed")
        return True

    except Exception as e:
        logger.error(f"âŒ End-to-end routing test failed: {e}")
        return False


async def test_error_handling():
    """Test error handling and resilience."""
    logger.info("ðŸ§ª Testing error handling...")

    try:
        from agents.dispatcher import AgentDispatcher
        from agents.exceptions import RaptorflowError, RoutingError, ValidationError

        # Test custom exceptions
        try:
            raise ValidationError("Test validation error")
        except ValidationError as e:
            assert hasattr(e, "message"), "ValidationError should have message"
            assert hasattr(e, "to_dict"), "ValidationError should have to_dict method"

        # Test dispatcher error handling
        dispatcher = AgentDispatcher()

        # Test invalid requests
        invalid_requests = [
            ("", "workspace", "user", "session"),  # Empty request
            ("valid", "", "user", "session"),  # Empty workspace
            ("valid", "workspace", "", "session"),  # Empty user
            ("valid", "workspace", "user", ""),  # Empty session
        ]

        for request, workspace, user, session in invalid_requests:
            try:
                await dispatcher.dispatch(request, workspace, user, session)
                assert False, f"Should fail for invalid request: {request}"
            except Exception:
                pass  # Expected to fail

        logger.info("âœ… Error handling test passed")
        return True

    except Exception as e:
        logger.error(f"âŒ Error handling test failed: {e}")
        return False


async def run_all_tests():
    """Run all tests and generate comprehensive report."""
    logger.info("ðŸš€ Starting comprehensive routing system tests...")
    logger.info("=" * 60)

    tests = [
        ("Configuration", test_configuration),
        ("Tool Registry", test_tool_registry),
        ("Routing Pipeline", test_routing_pipeline),
        ("Agent Dispatcher", test_agent_dispatcher),
        ("Workflow Graph", test_workflow_graph),
        ("Specialist Agents", test_specialist_agents),
        ("End-to-End Routing", test_end_to_end_routing),
        ("Error Handling", test_error_handling),
    ]

    results = []

    for test_name, test_func in tests:
        logger.info(f"\nðŸ“‹ Running {test_name} test...")
        try:
            start_time = datetime.now()
            success = await test_func()
            duration = (datetime.now() - start_time).total_seconds()

            results.append(
                {
                    "test": test_name,
                    "success": success,
                    "duration": duration,
                    "timestamp": datetime.now().isoformat(),
                }
            )

            status = "âœ… PASSED" if success else "âŒ FAILED"
            logger.info(f"{status} {test_name} ({duration:.2f}s)")

        except Exception as e:
            logger.error(f"âŒ {test_name} test crashed: {e}")
            results.append(
                {
                    "test": test_name,
                    "success": False,
                    "duration": 0,
                    "timestamp": datetime.now().isoformat(),
                    "error": str(e),
                }
            )

    # Generate report
    logger.info("\n" + "=" * 60)
    logger.info("ðŸ“Š TEST REPORT")
    logger.info("=" * 60)

    passed = sum(1 for r in results if r["success"])
    total = len(results)
    success_rate = passed / total * 100

    logger.info(f"Overall Success Rate: {success_rate:.1f}% ({passed}/{total})")

    for result in results:
        status = "âœ…" if result["success"] else "âŒ"
        duration_str = f" ({result['duration']:.2f}s)" if result["duration"] > 0 else ""
        logger.info(f"{status} {result['test']}{duration_str}")
        if not result["success"] and "error" in result:
            logger.info(f"   Error: {result['error']}")

    logger.info("=" * 60)

    if success_rate >= 80:
        logger.info("ðŸŽ‰ Routing system is PRODUCTION READY!")
        return True
    else:
        logger.error("ðŸš¨ Routing system needs fixes before production deployment")
        return False


if __name__ == "__main__":
    # Set environment variables for testing
    os.environ.setdefault("GCP_PROJECT_ID", "test-project")
    os.environ.setdefault("GCP_REGION", "us-central1")
    os.environ.setdefault("SUPABASE_URL", "https://test.supabase.co")
    os.environ.setdefault("SUPABASE_SERVICE_KEY", "test-key")
    os.environ.setdefault("UPSTASH_REDIS_URL", "https://test.redis.upstash.io")
    os.environ.setdefault("UPSTASH_REDIS_TOKEN", "test-token")
    os.environ.setdefault("ENCRYPTION_KEY", "test-encryption-key-32-chars-long")

    # Run tests
    success = asyncio.run(run_all_tests())
    sys.exit(0 if success else 1)
