"""
Test Suite: Saveetha Engineering College Startups
Search and scrape startup information using our tools
"""

import asyncio
import json

from free_web_search import free_search_engine
from ultra_fast_scraper import UltraFastScrapingStrategy, ultra_fast_scraper


async def search_saveetha_startups():
    """Search for Saveetha Engineering College startups"""
    print("ğŸ” Searching for Saveetha Engineering College startups...")
    print("=" * 60)

    search_queries = [
        "Saveetha Engineering College startups",
        "Saveetha student companies entrepreneurs",
        "Saveetha Engineering College innovation center",
        "startups from Saveetha Chennai",
        "Saveetha Engineering College student ventures",
    ]

    all_urls = []

    for query in search_queries:
        print(f"\nğŸ¯ Query: {query}")
        print("-" * 40)

        try:
            result = await free_search_engine.search(
                query=query,
                engines=["duckduckgo"],  # Use one engine for speed
                max_results=5,
            )

            print(f"âœ… Results: {result['total_results']}")

            # Extract relevant URLs
            for res in result["results"]:
                url = res["url"]
                title = res["title"]

                # Filter for relevant URLs
                if any(
                    keyword in title.lower() or keyword in url.lower()
                    for keyword in [
                        "saveetha",
                        "startup",
                        "entrepreneur",
                        "innovation",
                        "student",
                    ]
                ):
                    all_urls.append(
                        {
                            "url": url,
                            "title": title,
                            "snippet": res["snippet"],
                            "source": res["source"],
                        }
                    )
                    print(f"  ğŸ“„ {title}")
                    print(f"     ğŸ”— {url}")

        except Exception as e:
            print(f"âŒ Search failed: {str(e)}")

    # Remove duplicates
    unique_urls = []
    seen_urls = set()

    for item in all_urls:
        if item["url"] not in seen_urls:
            seen_urls.add(item["url"])
            unique_urls.append(item)

    print(f"\nğŸ¯ Found {len(unique_urls)} unique URLs to scrape")
    return unique_urls


async def scrape_startup_websites(urls):
    """Scrape startup websites using our ultra-fast scraper"""
    print("\nğŸš€ Scraping Startup Websites...")
    print("=" * 60)

    scraped_data = []

    for i, url_data in enumerate(urls[:5]):  # Limit to 5 for demo
        url = url_data["url"]
        title = url_data["title"]

        print(f"\nğŸ“„ [{i+1}/5] Scraping: {title}")
        print(f"ğŸ”— URL: {url}")

        try:
            # Use ultra-fast scraper with async strategy
            result = await ultra_fast_scraper.scrape_with_production_grade_handling(
                url=url,
                user_id="saveetha-startup-research",
                legal_basis="research",
                strategy=UltraFastScrapingStrategy.ASYNC,
            )

            if result.get("status") == "success":
                content = result.get("readable_text", "")
                content_length = result.get("content_length", 0)
                processing_time = result.get("processing_time", 0)

                print(f"âœ… Success: {content_length:,} chars in {processing_time:.2f}s")

                # Extract key information
                startup_info = {
                    "url": url,
                    "title": title,
                    "content_length": content_length,
                    "processing_time": processing_time,
                    "content_preview": (
                        content[:200] + "..." if len(content) > 200 else content
                    ),
                    "scraped_at": result.get("timestamp"),
                    "cost_tracking": result.get("cost_tracking", {}),
                    "production_metadata": result.get("production_metadata", {}),
                }

                scraped_data.append(startup_info)

                # Look for startup indicators
                startup_keywords = [
                    "startup",
                    "entrepreneur",
                    "innovation",
                    "venture",
                    "company",
                    "business",
                ]
                found_keywords = [
                    kw for kw in startup_keywords if kw.lower() in content.lower()
                ]

                if found_keywords:
                    print(f"  ğŸ¯ Startup indicators: {', '.join(found_keywords)}")

            else:
                print(f"âŒ Failed: {result.get('error', 'Unknown error')}")

        except Exception as e:
            print(f"âŒ Exception: {str(e)}")

    return scraped_data


async def analyze_startup_data(scraped_data):
    """Analyze scraped startup data"""
    print("\nğŸ“Š Analyzing Startup Data...")
    print("=" * 60)

    if not scraped_data:
        print("âŒ No data to analyze")
        return

    print(f"ğŸ“ˆ Successfully scraped {len(scraped_data)} websites")

    total_content = sum(item["content_length"] for item in scraped_data)
    avg_time = sum(item["processing_time"] for item in scraped_data) / len(scraped_data)
    total_cost = sum(
        item.get("cost_tracking", {}).get("estimated_cost", 0) for item in scraped_data
    )

    print(f"\nğŸ“Š Scraping Statistics:")
    print(f"  ğŸ“„ Total Content: {total_content:,} characters")
    print(f"  â±ï¸  Average Time: {avg_time:.2f}s per scrape")
    print(f"  ğŸ’° Total Cost: ${total_cost:.6f}")
    print(f"  ğŸš€ Average Speed: {total_content/avg_time:.0f} chars/second")

    print(f"\nğŸ¯ Startup Analysis:")

    for i, item in enumerate(scraped_data):
        print(f"\n{i+1}. {item['title']}")
        print(f"   ğŸ”— {item['url']}")
        print(
            f"   ğŸ“Š {item['content_length']:,} chars | â±ï¸ {item['processing_time']:.2f}s"
        )
        print(
            f"   ğŸ’° Cost: ${item.get('cost_tracking', {}).get('estimated_cost', 0):.6f}"
        )
        print(f"   ğŸ“ Preview: {item['content_preview']}")

        # Analyze content for startup characteristics
        content = item["content_preview"].lower()

        # Look for startup-related terms
        startup_terms = {
            "founded": "company founded",
            "team": "team information",
            "product": "product/service",
            "mission": "mission/vision",
            "contact": "contact information",
            "funding": "funding/investment",
            "technology": "technology stack",
            "market": "market/target",
        }

        found_terms = []
        for term, description in startup_terms.items():
            if term in content:
                found_terms.append(description)

        if found_terms:
            print(f"   ğŸ¯ Contains: {', '.join(found_terms)}")

    # Generate summary
    print(f"\nğŸ“‹ Summary:")
    print(f"  ğŸ” Search queries used: 5")
    print(f"  ğŸ“„ URLs found: {len(scraped_data)}")
    print(f"  âœ… Successfully scraped: {len(scraped_data)}")
    print(f"  ğŸ“Š Total content analyzed: {total_content:,} characters")
    print(f"  ğŸ’° Total scraping cost: ${total_cost:.6f}")
    print(f"  âš¡ Average performance: {avg_time:.2f}s per site")


async def main():
    """Main test suite"""
    print("ğŸš€ Saveetha Engineering College Startups - Test Suite")
    print("=" * 70)
    print("Using our own tools: Free Web Search + Ultra-Fast Scraper")
    print("=" * 70)

    try:
        # Step 1: Search for startup information
        urls = await search_saveetha_startups()

        if not urls:
            print("\nâŒ No URLs found to scrape")
            return

        # Step 2: Scrape the websites
        scraped_data = await scrape_startup_websites(urls)

        # Step 3: Analyze the results
        await analyze_startup_data(scraped_data)

        print(f"\nğŸ‰ Test Suite Completed Successfully!")
        print("=" * 70)
        print("âœ… Used our own tools:")
        print("  ğŸ” Free Web Search (DuckDuckGo)")
        print("  ğŸš€ Ultra-Fast Scraper (Async strategy)")
        print("  ğŸ’° Cost optimization tracking")
        print("  ğŸ“Š Production-grade error handling")
        print("  ğŸ¯ Visual intelligence extraction")

        # Save results
        output_file = "saveetha_startups_results.json"
        with open(output_file, "w") as f:
            json.dump(
                {
                    "search_results": urls,
                    "scraped_data": scraped_data,
                    "timestamp": asyncio.get_event_loop().time(),
                    "tools_used": ["free_web_search", "ultra_fast_scraper"],
                },
                f,
                indent=2,
            )

        print(f"  ğŸ“ Results saved to: {output_file}")

    except Exception as e:
        print(f"\nâŒ Test suite failed: {str(e)}")

    finally:
        await free_search_engine.close()


if __name__ == "__main__":
    asyncio.run(main())
