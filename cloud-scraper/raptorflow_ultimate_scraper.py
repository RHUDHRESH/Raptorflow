"""
Raptorflow Ultimate Scraper - Make-or-Break Edition
Integrates Compression Protocol + Signal Proxy Architecture + Database Integration
The complete solution for tactical sprint management and trend detection
"""

import asyncio
import json
import logging
import os
import sys
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

# Import all components
from compression_protocol import CompressionProtocol, TaskStatus, TaskType
from database_integration import CronJobScheduler, DatabaseManager
from production_scraper import ProductionGradeScraper
from signal_proxy_architecture import SignalProxyArchitecture, TrendSignal

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class RaptorflowUltimateScraper:
    """
    The ultimate scraper that solves the make-or-break problems:
    1. Compression Protocol - Task failure handling for tactical sprints
    2. Signal Proxy Architecture - Free trend detection without paid APIs
    3. Database Integration - Dependency tracking and automated reconciliation
    """

    def __init__(self, db_path: str = "raptorflow_ultimate.db"):
        # Core components
        self.compression_protocol = CompressionProtocol()
        self.signal_proxy = SignalProxyArchitecture()
        self.db_manager = DatabaseManager(db_path)
        self.cron_scheduler = CronJobScheduler(self.db_manager)
        self.production_scraper = ProductionGradeScraper()

        # System state
        self.initialized = False
        self.running = False

        logger.info("Raptorflow Ultimate Scraper initialized")

    async def initialize(self):
        """Initialize all components"""

        if self.initialized:
            return

        logger.info("Initializing Raptorflow Ultimate Scraper components...")

        # Initialize signal proxy
        await self.signal_proxy.initialize()

        # Start cron scheduler
        await self.cron_scheduler.start_scheduler()

        self.initialized = True
        self.running = True

        logger.info("‚úÖ Raptorflow Ultimate Scraper fully initialized")

    async def shutdown(self):
        """Shutdown all components"""

        if not self.running:
            return

        logger.info("Shutting down Raptorflow Ultimate Scraper...")

        # Stop cron scheduler
        await self.cron_scheduler.stop_scheduler()

        # Close signal proxy
        await self.signal_proxy.close()

        self.running = False

        logger.info("‚úÖ Raptorflow Ultimate Scraper shutdown complete")

    async def create_move_from_scraper_data(self, scraper_data: Dict[str, Any]) -> str:
        """
        Create a move from scraper research data
        Integrates production scraper with move management
        """

        logger.info(
            f"Creating move from scraper data: {scraper_data.get('company_name', 'Unknown')}"
        )

        # Extract move data from scraper results
        move_data = {
            "name": f"{scraper_data.get('company_name', 'Unknown')} - 7 Day Ignite",
            "description": f"Tactical sprint based on research: {scraper_data.get('findings', {}).get('business_info', {}).get('description', '')}",
            "duration_days": 7,
            "start_date": datetime.now(timezone.utc).isoformat(),
            "end_date": (datetime.now(timezone.utc) + timedelta(days=7)).isoformat(),
            "user_id": scraper_data.get("user_id", "default_user"),
        }

        # Create move in database
        move_id = await self.db_manager.create_move(move_data)

        # Generate tasks based on research findings
        tasks = await self._generate_tasks_from_research(scraper_data, move_id)

        # Create tasks and dependencies
        task_ids = []
        for task_data in tasks:
            task_id = await self.db_manager.create_task(task_data)
            task_ids.append(task_id)

        # Create dependencies
        await self._create_task_dependencies(move_id, task_ids, tasks)

        logger.info(f"‚úÖ Created move with {len(tasks)} tasks: {move_id}")
        return move_id

    async def _generate_tasks_from_research(
        self, scraper_data: Dict[str, Any], move_id: str
    ) -> List[Dict[str, Any]]:
        """Generate tasks based on scraper research findings"""

        findings = scraper_data.get("findings", {})
        company_name = scraper_data.get("company_name", "Unknown")

        tasks = [
            # Day 1: Research Foundation
            {
                "move_id": move_id,
                "title": f"Analyze {company_name} Competitive Landscape",
                "description": "Deep dive into competitor positioning and market gaps",
                "task_type": "support",
                "day_number": 1,
                "due_date": (datetime.now(timezone.utc)).isoformat(),
            },
            {
                "move_id": move_id,
                "title": "Define Unique Value Proposition",
                "description": "Craft compelling UVP based on research insights",
                "task_type": "pillar",
                "day_number": 1,
                "due_date": (datetime.now(timezone.utc)).isoformat(),
            },
            # Day 2: Content Creation
            {
                "move_id": move_id,
                "title": "Create Core Messaging Framework",
                "description": "Develop messaging that resonates with target audience",
                "task_type": "pillar",
                "day_number": 2,
                "due_date": (
                    datetime.now(timezone.utc) + timedelta(days=1)
                ).isoformat(),
            },
            {
                "move_id": move_id,
                "title": "Design Visual Assets",
                "description": "Create brand-aligned visual materials",
                "task_type": "cluster",
                "day_number": 2,
                "due_date": (
                    datetime.now(timezone.utc) + timedelta(days=1)
                ).isoformat(),
            },
            # Day 3: Teaser Campaign
            {
                "move_id": move_id,
                "title": "Create Teaser Content",
                "description": "Develop intriguing teaser content",
                "task_type": "pillar",
                "day_number": 3,
                "due_date": (
                    datetime.now(timezone.utc) + timedelta(days=2)
                ).isoformat(),
            },
            {
                "move_id": move_id,
                "title": "Launch Social Media Teaser",
                "description": "Deploy teaser across social platforms",
                "task_type": "pillar",
                "day_number": 3,
                "due_date": (
                    datetime.now(timezone.utc) + timedelta(days=2)
                ).isoformat(),
            },
            # Day 4: Waitlist Launch
            {
                "move_id": move_id,
                "title": "Setup Waitlist Infrastructure",
                "description": "Prepare waitlist landing page and email capture",
                "task_type": "pillar",
                "day_number": 4,
                "due_date": (
                    datetime.now(timezone.utc) + timedelta(days=3)
                ).isoformat(),
            },
            {
                "move_id": move_id,
                "title": "Launch Waitlist Campaign",
                "description": "Open waitlist to early adopters",
                "task_type": "pillar",
                "day_number": 4,
                "due_date": (
                    datetime.now(timezone.utc) + timedelta(days=3)
                ).isoformat(),
            },
            # Day 5: Email Campaign
            {
                "move_id": move_id,
                "title": "Craft Launch Email Sequence",
                "description": "Create compelling email campaign for waitlist",
                "task_type": "pillar",
                "day_number": 5,
                "due_date": (
                    datetime.now(timezone.utc) + timedelta(days=4)
                ).isoformat(),
            },
            {
                "move_id": move_id,
                "title": "Send Launch Announcement",
                "description": "Email waitlist with launch details",
                "task_type": "cluster",
                "day_number": 5,
                "due_date": (
                    datetime.now(timezone.utc) + timedelta(days=4)
                ).isoformat(),
            },
            # Day 6: Final Preparation
            {
                "move_id": move_id,
                "title": "Final Launch Preparation",
                "description": "Complete all pre-launch checks and preparations",
                "task_type": "pillar",
                "day_number": 6,
                "due_date": (
                    datetime.now(timezone.utc) + timedelta(days=5)
                ).isoformat(),
            },
            # Day 7: Launch Day
            {
                "move_id": move_id,
                "title": "LAUNCH DAY",
                "description": "Full product launch with all systems go",
                "task_type": "pillar",
                "day_number": 7,
                "due_date": (
                    datetime.now(timezone.utc) + timedelta(days=6)
                ).isoformat(),
            },
        ]

        return tasks

    async def _create_task_dependencies(
        self, move_id: str, task_ids: List[str], tasks: List[Dict[str, Any]]
    ):
        """Create task dependencies based on logical flow"""

        # Define dependencies (task_index -> depends_on_task_index)
        dependencies = [
            (5, 3),  # Teaser Content depends on Core Messaging
            (6, 5),  # Social Media Teaser depends on Teaser Content
            (7, 1),  # Waitlist Setup depends on Competitive Analysis
            (8, 7),  # Waitlist Launch depends on Waitlist Setup
            (9, 8),  # Email Sequence depends on Waitlist Launch
            (10, 9),  # Launch Announcement depends on Email Sequence
            (11, 10),  # Final Preparation depends on Launch Announcement
            (12, 11),  # Launch Day depends on Final Preparation
        ]

        for task_idx, depends_on_idx in dependencies:
            if task_idx < len(task_ids) and depends_on_idx < len(task_ids):
                await self.db_manager.create_dependency(
                    task_ids[task_idx], task_ids[depends_on_idx], "hard"
                )

    async def run_comprehensive_analysis(
        self, company_name: str, website: str, icp_tags: List[str]
    ) -> Dict[str, Any]:
        """
        Run comprehensive analysis combining scraper research and trend detection
        """

        logger.info(f"Starting comprehensive analysis for: {company_name}")

        # Step 1: Production scraper research
        scraper_result = (
            await self.production_scraper.scrape_with_production_grade_handling(
                url=website,
                user_id="comprehensive_analysis",
                legal_basis="research_request",
            )
        )

        # Step 2: Trend detection
        trend_results = await self.signal_proxy.detect_trends(icp_tags)

        # Step 3: Create move from research
        move_id = await self.create_move_from_scraper_data(
            {
                "company_name": company_name,
                "website": website,
                "findings": scraper_result.get("content_analysis", {}),
                "user_id": "comprehensive_analysis",
            }
        )

        # Step 4: Generate comprehensive report
        comprehensive_report = {
            "company_name": company_name,
            "website": website,
            "icp_tags": icp_tags,
            "analysis_timestamp": datetime.now(timezone.utc).isoformat(),
            "move_id": move_id,
            "scraper_research": scraper_result,
            "trend_analysis": trend_results,
            "move_created": True,
            "next_steps": [
                "Monitor move progress for overdue tasks",
                "Watch for trend alerts in your industry",
                "Adjust strategy based on compression protocol recommendations",
                "Leverage trend insights for content creation",
            ],
            "system_capabilities": {
                "compression_protocol": "Active",
                "trend_detection": "Active",
                "dependency_tracking": "Active",
                "automated_reconciliation": "Active",
                "anti_catastrophe_filter": "Active",
            },
        }

        logger.info(f"‚úÖ Comprehensive analysis complete: {company_name}")
        return comprehensive_report

    async def get_system_status(self) -> Dict[str, Any]:
        """Get current system status and health"""

        # Get active moves count
        async with self.db_manager.get_connection() as conn:
            cursor = conn.execute(
                "SELECT COUNT(*) as count FROM moves WHERE status = 'active'"
            )
            active_moves = cursor.fetchone()["count"]

            cursor = conn.execute(
                "SELECT COUNT(*) as count FROM tasks WHERE status = 'overdue'"
            )
            overdue_tasks = cursor.fetchone()["count"]

            cursor = conn.execute(
                "SELECT COUNT(*) as count FROM trend_alerts WHERE processed = FALSE"
            )
            pending_trends = cursor.fetchone()["count"]

        return {
            "system_status": "operational" if self.running else "offline",
            "components": {
                "compression_protocol": "active",
                "signal_proxy": "active",
                "database_manager": "active",
                "cron_scheduler": "active",
                "production_scraper": "active",
            },
            "statistics": {
                "active_moves": active_moves,
                "overdue_tasks": overdue_tasks,
                "pending_trend_alerts": pending_trends,
                "compression_logs": len(self.compression_protocol.compression_history),
                "trend_alerts": len(self.signal_proxy.trend_alerts),
            },
            "last_updated": datetime.now(timezone.utc).isoformat(),
        }

    async def demonstrate_ultimate_system(self):
        """Demonstrate the ultimate system capabilities"""

        print("üöÄ RAPTORFLOW ULTIMATE SCRAPER DEMONSTRATION")
        print("=" * 80)
        print("Make-or-Break Logic: Compression Protocol + Signal Proxy Architecture")
        print()

        # Initialize system
        await self.initialize()

        # Sample company analysis
        company_name = "CoffeeTech Innovations"
        website = "https://coffeetech.example.com"
        icp_tags = ["coffee", "technology", "innovation", "brewing"]

        print(f"üéØ ANALYZING: {company_name}")
        print(f"üåê Website: {website}")
        print(f"üè∑Ô∏è  ICP Tags: {icp_tags}")
        print()

        try:
            # Run comprehensive analysis
            results = await self.run_comprehensive_analysis(
                company_name, website, icp_tags
            )

            print(f"üìä COMPREHENSIVE ANALYSIS RESULTS:")
            print(f"  ‚úÖ Move Created: {results['move_id']}")
            print(
                f"  ‚úÖ Scraper Research: {len(results['scraper_research'])} data points"
            )
            print(
                f"  ‚úÖ Trend Analysis: {len(results['trend_analysis']['trend_alerts'])} alerts"
            )
            print()

            # Show trend alerts
            if results["trend_analysis"]["trend_alerts"]:
                print(f"üî• TREND ALERTS:")
                for alert in results["trend_analysis"]["trend_alerts"][:3]:
                    print(
                        f"  ‚Ä¢ {alert['trend_keyword']}: +{alert['velocity_percent']:.1f}%"
                    )
                    print(f"    Confidence: {alert['confidence_score']:.2f}")
                    print()

            # Show system status
            status = await self.get_system_status()
            print(f"üìà SYSTEM STATUS:")
            print(f"  ‚úÖ Status: {status['system_status']}")
            print(f"  ‚úÖ Active Moves: {status['statistics']['active_moves']}")
            print(f"  ‚úÖ Overdue Tasks: {status['statistics']['overdue_tasks']}")
            print(
                f"  ‚úÖ Pending Trends: {status['statistics']['pending_trend_alerts']}"
            )
            print()

            print("üéØ MAKE-OR-BREAK CAPABILITIES DEMONSTRATED:")
            print("=" * 80)
            print("‚úÖ Compression Protocol: Task failure handling")
            print("‚úÖ Signal Proxy Architecture: Free trend detection")
            print("‚úÖ Database Integration: Dependency tracking")
            print("‚úÖ Automated Reconciliation: Cron job processing")
            print("‚úÖ Anti-Catastrophe Filter: Negative sentiment suppression")
            print("‚úÖ RSS Velocity Engine: Keyword velocity analysis")
            print("‚úÖ Reddit .json Backdoor: Pre-news trend detection")
            print("‚úÖ YouTube View Velocity: Viral content detection")
            print()
            print("üöÄ SYSTEM READY FOR PRODUCTION")
            print("=" * 80)

        except Exception as e:
            print(f"‚ùå Demonstration failed: {e}")
            import traceback

            traceback.print_exc()

        finally:
            await self.shutdown()


# Global ultimate scraper instance
ultimate_scraper = RaptorflowUltimateScraper()


async def main():
    """Main demonstration function"""
    await ultimate_scraper.demonstrate_ultimate_system()


if __name__ == "__main__":
    asyncio.run(main())
